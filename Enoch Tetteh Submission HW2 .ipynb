{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder with Fashion Images\n",
    "In this notebook, we will be building a Convolutional Neural Network (CNN) for an Autoencoder (AE)\n",
    "using the Fashion MNIST dataset.\n",
    "The dataset consists of 60k and 10k training and test sets respectively. Each image is grayscale of size  28x28, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create directory to save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./AE_IMAGES'):\n",
    "    os.mkdir('./AE_IMAGES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to count number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert vector to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1) \n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data', download=True,\n",
    "                            train=True, transform=img_transform)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./data', \n",
    "                           train=False, transform=img_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=1000, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model architecture and reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=5, stride=3, padding=1),\n",
    "            nn.LeakyReLU(),   \n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=2, stride=2, padding=1),   \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([8, 16, 3, 3])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 16, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 8, 5, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 1, 2, 2])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "### Input Parameters\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "\n",
    "for i in range(len(params)):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5729\n"
     ]
    }
   ],
   "source": [
    "### Model Total Parameters\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 10, 10]             160\n",
      "         LeakyReLU-2           [-1, 16, 10, 10]               0\n",
      "         AvgPool2d-3             [-1, 16, 5, 5]               0\n",
      "            Conv2d-4              [-1, 8, 3, 3]           1,160\n",
      "         LeakyReLU-5              [-1, 8, 3, 3]               0\n",
      "         AvgPool2d-6              [-1, 8, 2, 2]               0\n",
      "   ConvTranspose2d-7             [-1, 16, 5, 5]           1,168\n",
      "         LeakyReLU-8             [-1, 16, 5, 5]               0\n",
      "   ConvTranspose2d-9            [-1, 8, 15, 15]           3,208\n",
      "        LeakyReLU-10            [-1, 8, 15, 15]               0\n",
      "  ConvTranspose2d-11            [-1, 1, 28, 28]              33\n",
      "             Tanh-12            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 5,729\n",
      "Trainable params: 5,729\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Model Summary\n",
    "summary(model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "l2_regularization = 1e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=l2_regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train autoencoder (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], loss:0.1660\n",
      "epoch [2/20], loss:0.1318\n",
      "epoch [3/20], loss:0.1153\n",
      "epoch [4/20], loss:0.1140\n",
      "epoch [5/20], loss:0.0991\n",
      "epoch [6/20], loss:0.1000\n",
      "epoch [7/20], loss:0.1039\n",
      "epoch [8/20], loss:0.1039\n",
      "epoch [9/20], loss:0.0876\n",
      "epoch [10/20], loss:0.0920\n",
      "epoch [11/20], loss:0.0946\n",
      "epoch [12/20], loss:0.0947\n",
      "epoch [13/20], loss:0.0857\n",
      "epoch [14/20], loss:0.0814\n",
      "epoch [15/20], loss:0.0939\n",
      "epoch [16/20], loss:0.0849\n",
      "epoch [17/20], loss:0.0879\n",
      "epoch [18/20], loss:0.0858\n",
      "epoch [19/20], loss:0.0826\n",
      "epoch [20/20], loss:0.0880\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        img.requires_grad_()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)  \n",
    "        loss = criterion(output, img.data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f'epoch [{epoch + 1}/{num_epochs}], loss:{loss.item():.4f}')  \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './AE_IMAGES/{}.png'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIMS DL",
   "language": "python",
   "name": "aims-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
